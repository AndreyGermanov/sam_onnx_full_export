{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/segment-anything.git\r\n",
      "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-iu3vc3ae\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-iu3vc3ae\r\n",
      "  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: onnx in /home/andrey/anaconda3/lib/python3.9/site-packages (1.12.0)\r\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/andrey/anaconda3/lib/python3.9/site-packages (from onnx) (1.25.2)\r\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /home/andrey/anaconda3/lib/python3.9/site-packages (from onnx) (3.20.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.2.1 in /home/andrey/anaconda3/lib/python3.9/site-packages (from onnx) (4.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install git+https://github.com/facebookresearch/segment-anything.git\n",
    "!pip3 install onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from segment_anything import sam_model_registry\n",
    "from segment_anything.utils.onnx import SamOnnxModel\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /home/andrey/anaconda3/lib/python3.9/site-packages (3.2)\r\n",
      "100% [..................................................] 375042383 / 375042383\r\n",
      "Saved under sam_vit_b_01ec64 (1).pth\r\n"
     ]
    }
   ],
   "source": [
    "# Download SAM model checkpoint\n",
    "!pip install wget\n",
    "!python -m wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Load SAM model\n",
    "sam = sam_model_registry[\"vit_b\"](checkpoint=\"./sam_vit_b_01ec64.pth\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrey/anaconda3/lib/python3.9/site-packages/segment_anything/modeling/image_encoder.py:258: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if pad_h > 0 or pad_w > 0:\n",
      "/home/andrey/anaconda3/lib/python3.9/site-packages/segment_anything/modeling/image_encoder.py:304: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  max_rel_dist = int(2 * max(q_size, k_size) - 1)\n",
      "/home/andrey/anaconda3/lib/python3.9/site-packages/segment_anything/modeling/image_encoder.py:304: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  max_rel_dist = int(2 * max(q_size, k_size) - 1)\n",
      "/home/andrey/anaconda3/lib/python3.9/site-packages/segment_anything/modeling/image_encoder.py:306: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if rel_pos.shape[0] != max_rel_dist:\n",
      "/home/andrey/anaconda3/lib/python3.9/site-packages/segment_anything/modeling/image_encoder.py:318: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  q_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\n",
      "/home/andrey/anaconda3/lib/python3.9/site-packages/segment_anything/modeling/image_encoder.py:319: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  k_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\n",
      "/home/andrey/anaconda3/lib/python3.9/site-packages/segment_anything/modeling/image_encoder.py:320: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)\n",
      "/home/andrey/anaconda3/lib/python3.9/site-packages/segment_anything/modeling/image_encoder.py:287: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if Hp > H or Wp > W:\n",
      "[W NNPACK.cpp:53] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "/home/andrey/anaconda3/lib/python3.9/site-packages/torch/onnx/utils.py:687: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/home/andrey/anaconda3/lib/python3.9/site-packages/torch/onnx/utils.py:1178: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "# Export images encoder from SAM model to ONNX\n",
    "torch.onnx.export(\n",
    "    f=\"vit_b_encoder.onnx\",\n",
    "    model=sam.image_encoder,\n",
    "    args=torch.randn(1, 3, 1024, 1024),\n",
    "    input_names=[\"images\"],\n",
    "    output_names=[\"embeddings\"],\n",
    "    dynamic_axes={\n",
    "        'images':{0:'batch_size'},\n",
    "        'embeddings':{0:'batch_size'}\n",
    "    },\n",
    "    export_params=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrey/anaconda3/lib/python3.9/site-packages/segment_anything/modeling/transformer.py:232: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  attn = attn / math.sqrt(c_per_head)\n",
      "/home/andrey/anaconda3/lib/python3.9/site-packages/segment_anything/utils/onnx.py:97: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  score_reweight = torch.tensor(\n",
      "/home/andrey/anaconda3/lib/python3.9/site-packages/torch/onnx/symbolic_opset9.py:5408: UserWarning: Exporting aten::index operator of advanced indexing in opset 17 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n",
      "/home/andrey/anaconda3/lib/python3.9/site-packages/torch/onnx/_internal/jit_utils.py:258: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
      "/home/andrey/anaconda3/lib/python3.9/site-packages/torch/onnx/utils.py:687: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
      "/home/andrey/anaconda3/lib/python3.9/site-packages/torch/onnx/utils.py:1178: UserWarning: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function. (Triggered internally at ../torch/csrc/jit/passes/onnx/shape_type_inference.cpp:1884.)\n",
      "  _C._jit_pass_onnx_graph_shape_type_inference(\n"
     ]
    }
   ],
   "source": [
    "# Export mask decoder from SAM model to ONNX\n",
    "onnx_model = SamOnnxModel(sam, return_single_mask=True)\n",
    "embed_dim = sam.prompt_encoder.embed_dim\n",
    "embed_size = sam.prompt_encoder.image_embedding_size\n",
    "mask_input_size = [4 * x for x in embed_size]\n",
    "dummy_inputs = {\n",
    "    \"image_embeddings\": torch.randn(1, embed_dim, *embed_size, dtype=torch.float),\n",
    "    \"point_coords\": torch.randint(low=0, high=1024, size=(1, 5, 2), dtype=torch.float),\n",
    "    \"point_labels\": torch.randint(low=0, high=4, size=(1, 5), dtype=torch.float),\n",
    "    \"mask_input\": torch.randn(1, 1, *mask_input_size, dtype=torch.float),\n",
    "    \"has_mask_input\": torch.tensor([1], dtype=torch.float),\n",
    "    \"orig_im_size\": torch.tensor([1500, 2250], dtype=torch.float),\n",
    "}\n",
    "output_names = [\"masks\", \"iou_predictions\", \"low_res_masks\"]\n",
    "torch.onnx.export(\n",
    "    f=\"vit_b_decoder.onnx\",\n",
    "    model=onnx_model,\n",
    "    args=tuple(dummy_inputs.values()),\n",
    "    input_names=list(dummy_inputs.keys()),\n",
    "    output_names=output_names,\n",
    "    dynamic_axes={\n",
    "        \"point_coords\": {1: \"num_points\"},\n",
    "        \"point_labels\": {1: \"num_points\"}\n",
    "    },\n",
    "    export_params=True,\n",
    "    opset_version=17,\n",
    "    do_constant_folding=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
